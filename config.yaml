# DevOpsGPT Configuration

llm:
  # Default provider - can be "openai" or "ollama"
  provider: "ollama"
  
  # Ollama settings (primary)
  base_url: "http://localhost:11434"
  model: "llama2"
  max_tokens: 500

  # OpenAI settings (optional fallback)
  fallback_provider: true
  openai_api_key: "${OPENAI_API_KEY}"
  openai_model: "gpt-4"

plugins:
  enabled:
    - troubleshooting
    - cicd
    - cost_analysis
    - monitoring

security:
  require_confirmation: true
  dry_run_by_default: true
  audit_logging: true
  allowed_actions:
    - logs
    - status
    - list
    - describe
    - get
  restricted_actions:
    - delete
    - remove
    - terminate

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "devops_gpt.log"

monitoring:
  prometheus:
    enabled: true
    port: 9090
  grafana:
    enabled: true
    url: "http://localhost:3000"